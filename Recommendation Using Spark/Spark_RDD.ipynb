{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5NcBtmT20v8",
        "outputId": "57608e04-cf97-4d93-9617-1c7f5b8f8cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ca66e075fe94e3c7c6986110fd3030f93831682739f1da43c6cf2a9f92a45613\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khi ta tạo một SparkContext, Tham số Main gán một tên do ta chọn cho các công việc Spark được thực thi. Tham số master chỉ ra rằng các công việc Spark sẽ chạy ở chế độ cục bộ, sử dụng hai luồng. Điều này có nghĩa là các công việc Spark của chúng ta thực sự không chạy trên một cụm, mà thay vào đó chạy trong một quy trình đơn lẻ trên máy cục bộ. Ta lập trình các công việc Spark theo cùng một cách dù chúng chạy ở chế độ cục bộ hay trên một cụm - sự khác biệt chính giữa chế độ cục bộ và cụm, tất nhiên, là hiệu suất.\n"
      ],
      "metadata": {
        "id": "56Az5HCO-4zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"Main\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ],
      "metadata": {
        "id": "5D8Pqf9224vZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trong đoạn mã tiếp theo, parallelize() và map() là các phép biến đổi của Spark, và reduce() là action trong Spark.Chạy code bên dưới, một job Spark sẽ được tạo và thực thi, và nó sẽ in ra một estimation của e khi hoàn thành."
      ],
      "metadata": {
        "id": "f2O2j8K1_m4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "n= 10000\n",
        "inverse_e = sc.parallelize(range(0, n)).map(lambda x: ((-1)**x) * (1 / math.factorial(x))).reduce(lambda x,y:x+y)\n",
        "e = 1 / inverse_e\n",
        "print(\"e = \", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV4o5W9t_0p8",
        "outputId": "5710d16c-41f3-4b9b-d576-0c9ce5d393cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e =  2.718281828459044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nó bắt đầu bằng cách sử dụng SparkContext (sc), cho Python biết rằng chúng ta đang sử dụng các chức năng của Spark được triển khai trong lớp SparkContext. Sau đó, range (0, n) được parallize, nghĩa là, nó được \"chuyển đổi\" thành một RDD, một tập hợp parallize các đối tượng trong Spark mà dữ liệu của nó được phân vùng trên nhiều cụm (trong trường hợp này là một nút đơn lẻ).\n",
        "\n",
        "Tiếp theo, RDD được chuyển đổi thêm (tạo ra một RDD khác) bằng cách sử dụng thao tác map, trong ví dụ tính $e$, chuyển đổi các số nguyên thành các hạng tử riêng lẻ của dãy số đã cho. Điều đáng nói là RDD chưa thực hiện bất kỳ chuyển đổi nào cho đến thời điểm hiện tại. Một chương trình Spark duy trì thứ tự thực thi dưới dạng DAG (Directed Acyclic Graph) trong đó mỗi đỉnh biểu thị một RDD và mỗi cạnh trước đó biểu thị thao tác đã tạo ra RDD đó. Nói cách khác, mỗi RDD duy trì một con trỏ đến RDD cha của nó duy trì phép biến đổi đã tạo ra nó. Điều này được gọi là phả hệ (lineage).\n",
        "\n",
        "Quay trở lại với code estimation $e$, RDD được ánh xạ sau đó được giảm xuống một giá trị duy nhất bằng cách tính tổng chạy của từng hạng tử liên tiếp, tổng hợp tất cả các hạng tử của dãy số đã cho. reduce() này là action nộp DAG cho bộ lập lịch DAG trong Spark và dẫn đến việc tính toán các RDD đã chỉ định. Cuối cùng, chúng ta lấy nghịch đảo của dãy số sử dụng Python để ước tính giá trị của e."
      ],
      "metadata": {
        "id": "ZSaltSXo_6OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/Shakespeare.txt\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/simple_tokenize.py"
      ],
      "metadata": {
        "id": "rV1j1V8x28qN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"Shakespeare.txt\""
      ],
      "metadata": {
        "id": "QLomlHZk2_T0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "\n",
        "# Returns the count of distinct tokens in the `Shakespeare.txt` dataset\n",
        "def count_distinct_tokens(sc, file):\n",
        "\n",
        "    text_file = sc.textFile(file)\n",
        "\n",
        "    # Works and is efficient because of combinining although we don't want sum for now.\n",
        "    text_take = text_file.flatMap(lambda x: simple_tokenize(x)).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).count()\n",
        "    return text_take"
      ],
      "metadata": {
        "id": "P-dQo32f3BVv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_distinct_tokens(sc, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZnkciqE3DIz",
        "outputId": "063d6cbb-cd5c-464f-fd04-5de5507fcdff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25975"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}